{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adagrad # Adam doesn't currently support autograd with embedding layers\n",
    "import numpy as np\n",
    "from tflearn.data_utils import pad_sequences\n",
    "import pickle as pkl\n",
    "from keras.utils import to_categorical\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURE = 10000\n",
    "SENTENCE_LEN = 300\n",
    "NGRAME_RANGE = 1\n",
    "EMBEDDING_DIMS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "def load_imdb_data():\n",
    "    def load_data(path='imdb.npz', num_words=None, skip_top=0, seed=113, start_char=1, oov_char=2, index_from=3):\n",
    "        # 1. load data\n",
    "        with np.load(path) as f:\n",
    "            x_train, labels_train = f['x_train'], f['y_train']\n",
    "            x_test, labels_test = f['x_test'], f['y_test']\n",
    "\n",
    "        # 2. shuffle train/test\n",
    "        np.random.seed(seed)\n",
    "        indices = np.arange(len(x_train))\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = x_train[indices]\n",
    "        labels_train = labels_train[indices]\n",
    "\n",
    "        indices = np.arange(len(x_test))\n",
    "        np.random.shuffle(indices)\n",
    "        x_test = x_test[indices]\n",
    "        labels_test = labels_test[indices]\n",
    "\n",
    "        xs = np.concatenate([x_train, x_test])\n",
    "        labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "        # 保留前3个index\n",
    "        if start_char is not None:\n",
    "            xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "        elif index_from:\n",
    "            xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "        if not num_words:\n",
    "            num_words = max([max(x) for x in xs])\n",
    "\n",
    "        # by convention, use 2 as OOV word\n",
    "        # reserve 'index_from' (=3 by default) characters:\n",
    "        # 0 (padding), 1 (start), 2 (OOV)\n",
    "        if oov_char is not None:\n",
    "            xs = [[w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs]\n",
    "        else:\n",
    "            xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n",
    "\n",
    "        idx = len(x_train)\n",
    "        x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "        x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "    \n",
    "    global MAX_FEATURE\n",
    "    print('MAX_FEATURE:', MAX_FEATURE)\n",
    "        \n",
    "    \n",
    "    # 1. load original data\n",
    "    print('loading data...')\n",
    "    (trainX, trainY), (testX, testY) = load_data(num_words=MAX_FEATURE)\n",
    "    print('train_data length:',len(trainX))\n",
    "    print('test_data length:',len(testX))\n",
    "    \n",
    "    # 2. add n-gram\n",
    "    if NGRAME_RANGE > 1:\n",
    "        print('Adding {}-gram features'.format(NGRAME_RANGE))\n",
    "        # Create set of unique n-gram from the training set.\n",
    "        ngram_set = set()\n",
    "        for input_list in trainX:\n",
    "            for i in range(2, NGRAME_RANGE + 1):\n",
    "                set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "                ngram_set.update(set_of_ngram)\n",
    "\n",
    "        # Dictionary mapping n-gram token to a unique integer.\n",
    "        # Integer values are greater than max_features in order\n",
    "        # to avoid collision with existing features.\n",
    "        print('MAX_FEATURE:', MAX_FEATURE)\n",
    "        start_index = MAX_FEATURE + 1\n",
    "        token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "        indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "        # max_features is the highest integer that could be found in the dataset.\n",
    "        MAX_FEATURE = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "        # Augmenting x_train and x_test with n-grams features\n",
    "        trainX = add_ngram(trainX, token_indice, NGRAME_RANGE)\n",
    "        testX = add_ngram(testX, token_indice, NGRAME_RANGE)\n",
    "        print('Average train sequence length: {}'.format(np.mean(list(map(len, trainX)), dtype=int)))\n",
    "        print('Average test sequence length: {}'.format(np.mean(list(map(len, testX)), dtype=int)))\n",
    "\n",
    "\n",
    "    # 3.Data preprocessing      Sequence padding\n",
    "    print(\"start padding & transform to one hot...\")\n",
    "    trainX = pad_sequences(trainX, maxlen=SENTENCE_LEN, value=0.)  # padding to max length\n",
    "    testX = pad_sequences(testX, maxlen=SENTENCE_LEN, value=0.)  # padding to max length\n",
    "    print('x_train shape:', trainX.shape)\n",
    "    print('x_test shape:', testX.shape)\n",
    "\n",
    "    print(\"end padding & transform to one hot...\")\n",
    "    return (trainX,trainY),(testX,testY)\n",
    "#     return (trainX, to_categorical(trainY)), (testX, to_categorical(testY))\n",
    "\n",
    "\n",
    "\n",
    "def lazy_load_imdb_data(ngram_range=1, max_features=20000, maxlen=400):\n",
    "    filename = \"-\".join([\"data\", str(ngram_range), str(max_features), str(maxlen)])\n",
    "    filename += \".pkl\"\n",
    "    print(filename)\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"rb\") as source:\n",
    "            print('lazy loading...')\n",
    "            data = pkl.load(source)\n",
    "            print(\"Lazy load successful\")\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "#         data = fetch_imdb_data(ngram_range, max_features, maxlen)\n",
    "        data = load_imdb_data()\n",
    "        with open(filename, \"wb\") as target:\n",
    "            pkl.dump(data, target)\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2index_dict(path='imdb_word_index.json'):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index = get_word2index_dict()  # {word:index}\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained embeddings GloVe is loading...\n",
      "Found 400000 word vectors in GloVe embedding\n"
     ]
    }
   ],
   "source": [
    "def create_glove_embeddings():\n",
    "    print('Pretrained embeddings GloVe is loading...')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open('/liruishaer/Work2/NLP_models/glove.6B/glove.6B.%id.txt' % EMBEDDING_DIMS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors in GloVe embedding' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((MAX_FEATURE, EMBEDDING_DIMS))\n",
    "    #embedding_matrix = torch.zeros(MAX_FEATURE, EMBEDDING_DIMS)\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_FEATURE:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            #embedding_matrix[i,:] = torch.from_numpy(embedding_vector)\n",
    "    \n",
    "    embedding_matrix = torch.tensor(embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-1-20000-400.pkl\n",
      "MAX_FEATURE: 10000\n",
      "loading data...\n",
      "train_data length: 25000\n",
      "test_data length: 25000\n",
      "start padding & transform to one hot...\n",
      "x_train shape: (25000, 300)\n",
      "x_test shape: (25000, 300)\n",
      "end padding & transform to one hot...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = lazy_load_imdb_data()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        print(f\"x.shape: {self.x.shape}\")\n",
    "        print(f\"y.shape: {self.y.shape}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         y_i = torch.FloatTensor(self.y[idx, :])\n",
    "#         x_i = torch.LongTensor(self.x[idx].tolist())\n",
    "        \n",
    "        y_i = torch.LongTensor([self.y[idx]])\n",
    "        x_i = torch.LongTensor(self.x[idx].tolist())\n",
    "\n",
    "        return {\"x\":x_i, \"y\":y_i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch in tqdm(testing_loader):\n",
    "        batch_x = Variable(batch[\"x\"])\n",
    "        outputs = model(batch_x)\n",
    "        batch_y = Variable(batch['y'].reshape(1,-1).squeeze(0))\n",
    "        test_loss += binary_loss(outputs, batch_y)\n",
    "\n",
    "        prediction = outputs.data.max(1, keepdim=True)[1]\n",
    "        #label = batch_y.data.max(1, keepdim=True)[1]\n",
    "        label = batch['y'].data\n",
    "        correct += prediction.eq(torch.LongTensor(label)).sum()\n",
    "    \n",
    "    test_loss /= len(testing_loader.dataset)\n",
    "    accuracy = 100. * correct / len(testing_loader.dataset)\n",
    "    print(f'Average Test loss: {test_loss.data[0]}')\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    print('-'*10)\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    #for batch in tqdm(training_loader):\n",
    "    for batch in training_loader:\n",
    "        # Get the inputs and wrap as Variables\n",
    "        batch_x = Variable(batch[\"x\"])\n",
    "        #batch_y = Variable(batch[\"y\"])\n",
    "        batch_y = Variable(batch['y'].reshape(1,-1).squeeze(0))\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "#         print(\"outputs.shape:\",outputs.shape)\n",
    "#         print(\"batch_y.shape:\",batch_y.shape)\n",
    "#         print(\"outputs\",outputs)\n",
    "#         print(\"batch_y\",batch_y)\n",
    "        \n",
    "        loss = binary_loss(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (25000, 300)\n",
      "y.shape: (25000,)\n",
      "x.shape: (25000, 300)\n",
      "y.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "training_data = MyData(x_train, y_train)\n",
    "testing_data = MyData(x_test, y_test)\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=5)\n",
    "testing_loader = DataLoader(testing_data, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFastText(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_features, embedding_dims, maxlen, num_classes=2):\n",
    "        super(TorchFastText, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.maxlen = maxlen\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "#         self.embeds = nn.EmbeddingBag(max_features, embedding_dims)\n",
    "#         self.linear = nn.Linear(self.embedding_dims, self.num_classes)\n",
    "        \n",
    "        self.embeds = nn.Embedding(max_features, embedding_dims)\n",
    "        self.linear = nn.Linear(self.embedding_dims, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded_sentence = self.embeds(x)\n",
    "        print('embeded.shape:',embedded_sentence.shape)\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded_sentence, (embedded_sentence.shape[1], 1)).squeeze(1)         \n",
    "        print('pooled.shape:',pooled.shape)\n",
    "        \n",
    "        predicted = self.linear(pooled)\n",
    "        print('predicted.shape:',predicted.shape)\n",
    "        \n",
    "        return predicted\n",
    "\n",
    "\n",
    "model = TorchFastText(MAX_FEATURE, EMBEDDING_DIMS, SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.5569,  0.3345,  0.0683,  ...,  0.0375, -0.5230,  0.5233],\n",
       "        [ 0.0453,  0.3146,  0.6410,  ..., -0.1689, -1.0540,  0.4726],\n",
       "        [ 0.3994,  0.5463,  0.3801,  ...,  0.4579, -0.1834,  0.1226]])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeds.weight.data.copy_(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#binary_loss = nn.BCELoss()\n",
    "binary_loss = nn.CrossEntropyLoss()\n",
    "# optimizer = Adagrad(model.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch: 1\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n",
      "embeded.shape: torch.Size([5, 300, 100])\n",
      "pooled.shape: torch.Size([5, 100])\n",
      "predicted.shape: torch.Size([5, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-336-6aebb3f0bd7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#     test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# test()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-331-084b94215482>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train(i)\n",
    "#     test()\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
