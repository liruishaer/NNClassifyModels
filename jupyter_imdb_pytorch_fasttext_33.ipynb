{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD, Adagrad # Adam doesn't currently support autograd with embedding layers\n",
    "import numpy as np\n",
    "from tflearn.data_utils import pad_sequences\n",
    "import pickle as pkl\n",
    "from keras.utils import to_categorical\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURE = 10000\n",
    "SENTENCE_LEN = 300\n",
    "NGRAME_RANGE = 1\n",
    "EMBEDDING_DIMS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_set(input_list, ngram_value=2):\n",
    "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "    new_sequences = []\n",
    "    for input_list in sequences:\n",
    "        new_list = input_list[:]\n",
    "        for ngram_value in range(2, ngram_range + 1):\n",
    "            for i in range(len(new_list) - ngram_value + 1):\n",
    "                ngram = tuple(new_list[i:i + ngram_value])\n",
    "                if ngram in token_indice:\n",
    "                    new_list.append(token_indice[ngram])\n",
    "        new_sequences.append(new_list)\n",
    "\n",
    "    return new_sequences\n",
    "\n",
    "def load_imdb_data():\n",
    "    def load_data(path='imdb.npz', num_words=None, skip_top=0, seed=113, start_char=1, oov_char=2, index_from=3):\n",
    "        # 1. load data\n",
    "        with np.load(path) as f:\n",
    "            x_train, labels_train = f['x_train'], f['y_train']\n",
    "            x_test, labels_test = f['x_test'], f['y_test']\n",
    "\n",
    "        # 2. shuffle train/test\n",
    "        np.random.seed(seed)\n",
    "        indices = np.arange(len(x_train))\n",
    "        np.random.shuffle(indices)\n",
    "        x_train = x_train[indices]\n",
    "        labels_train = labels_train[indices]\n",
    "\n",
    "        indices = np.arange(len(x_test))\n",
    "        np.random.shuffle(indices)\n",
    "        x_test = x_test[indices]\n",
    "        labels_test = labels_test[indices]\n",
    "\n",
    "        xs = np.concatenate([x_train, x_test])\n",
    "        labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "        # 保留前3个index\n",
    "        if start_char is not None:\n",
    "            xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "        elif index_from:\n",
    "            xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "        if not num_words:\n",
    "            num_words = max([max(x) for x in xs])\n",
    "\n",
    "        # by convention, use 2 as OOV word\n",
    "        # reserve 'index_from' (=3 by default) characters:\n",
    "        # 0 (padding), 1 (start), 2 (OOV)\n",
    "        if oov_char is not None:\n",
    "            xs = [[w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs]\n",
    "        else:\n",
    "            xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n",
    "\n",
    "        idx = len(x_train)\n",
    "        x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "        x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "\n",
    "        return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "    \n",
    "    global MAX_FEATURE\n",
    "    print('MAX_FEATURE:', MAX_FEATURE)\n",
    "        \n",
    "    \n",
    "    # 1. load original data\n",
    "    print('loading data...')\n",
    "    (trainX, trainY), (testX, testY) = load_data(num_words=MAX_FEATURE)\n",
    "    print('train_data length:',len(trainX))\n",
    "    print('test_data length:',len(testX))\n",
    "    \n",
    "    # 2. add n-gram\n",
    "    if NGRAME_RANGE > 1:\n",
    "        print('Adding {}-gram features'.format(NGRAME_RANGE))\n",
    "        # Create set of unique n-gram from the training set.\n",
    "        ngram_set = set()\n",
    "        for input_list in trainX:\n",
    "            for i in range(2, NGRAME_RANGE + 1):\n",
    "                set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "                ngram_set.update(set_of_ngram)\n",
    "\n",
    "        # Dictionary mapping n-gram token to a unique integer.\n",
    "        # Integer values are greater than max_features in order\n",
    "        # to avoid collision with existing features.\n",
    "        print('MAX_FEATURE:', MAX_FEATURE)\n",
    "        start_index = MAX_FEATURE + 1\n",
    "        token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "        indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "        # max_features is the highest integer that could be found in the dataset.\n",
    "        MAX_FEATURE = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "        # Augmenting x_train and x_test with n-grams features\n",
    "        trainX = add_ngram(trainX, token_indice, NGRAME_RANGE)\n",
    "        testX = add_ngram(testX, token_indice, NGRAME_RANGE)\n",
    "        print('Average train sequence length: {}'.format(np.mean(list(map(len, trainX)), dtype=int)))\n",
    "        print('Average test sequence length: {}'.format(np.mean(list(map(len, testX)), dtype=int)))\n",
    "\n",
    "\n",
    "    # 3.Data preprocessing      Sequence padding\n",
    "    print(\"start padding & transform to one hot...\")\n",
    "    trainX = pad_sequences(trainX, maxlen=SENTENCE_LEN, value=0.)  # padding to max length\n",
    "    testX = pad_sequences(testX, maxlen=SENTENCE_LEN, value=0.)  # padding to max length\n",
    "    print('x_train shape:', trainX.shape)\n",
    "    print('x_test shape:', testX.shape)\n",
    "\n",
    "    print(\"end padding & transform to one hot...\")\n",
    "    return (trainX,trainY),(testX,testY)\n",
    "#     return (trainX, to_categorical(trainY)), (testX, to_categorical(testY))\n",
    "\n",
    "\n",
    "\n",
    "def lazy_load_imdb_data(ngram_range=1, max_features=20000, maxlen=400):\n",
    "    filename = \"-\".join([\"data\", str(ngram_range), str(max_features), str(maxlen)])\n",
    "    filename += \".pkl\"\n",
    "    print(filename)\n",
    "\n",
    "    try:\n",
    "        with open(filename, \"rb\") as source:\n",
    "            print('lazy loading...')\n",
    "            data = pkl.load(source)\n",
    "            print(\"Lazy load successful\")\n",
    "            return data\n",
    "    except FileNotFoundError:\n",
    "#         data = fetch_imdb_data(ngram_range, max_features, maxlen)\n",
    "        data = load_imdb_data()\n",
    "        with open(filename, \"wb\") as target:\n",
    "            pkl.dump(data, target)\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2index_dict(path='imdb_word_index.json'):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# A dictionary mapping words to an integer index\n",
    "word_index = get_word2index_dict()  # {word:index}\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained embeddings GloVe is loading...\n",
      "Found 400000 word vectors in GloVe embedding\n"
     ]
    }
   ],
   "source": [
    "def create_glove_embeddings():\n",
    "    print('Pretrained embeddings GloVe is loading...')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open('/liruishaer/Work2/NLP_models/glove.6B/glove.6B.%id.txt' % EMBEDDING_DIMS)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors in GloVe embedding' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((MAX_FEATURE, EMBEDDING_DIMS))\n",
    "    #embedding_matrix = torch.zeros(MAX_FEATURE, EMBEDDING_DIMS)\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_FEATURE:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            #embedding_matrix[i,:] = torch.from_numpy(embedding_vector)\n",
    "    \n",
    "    embedding_matrix = torch.tensor(embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-1-20000-400.pkl\n",
      "MAX_FEATURE: 10000\n",
      "loading data...\n",
      "train_data length: 25000\n",
      "test_data length: 25000\n",
      "start padding & transform to one hot...\n",
      "x_train shape: (25000, 300)\n",
      "x_test shape: (25000, 300)\n",
      "end padding & transform to one hot...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = lazy_load_imdb_data()\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_glove_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyData(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        print(f\"x.shape: {self.x.shape}\")\n",
    "        print(f\"y.shape: {self.y.shape}\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "#         y_i = torch.FloatTensor(self.y[idx, :])\n",
    "#         x_i = torch.LongTensor(self.x[idx].tolist())\n",
    "        \n",
    "        y_i = torch.LongTensor([self.y[idx]])\n",
    "        x_i = torch.LongTensor(self.x[idx].tolist())\n",
    "\n",
    "        return {\"x\":x_i, \"y\":y_i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch in tqdm(testing_loader):\n",
    "        batch_x = Variable(batch[\"x\"])\n",
    "        outputs = model(batch_x)\n",
    "        batch_y = Variable(batch['y'].reshape(1,-1).squeeze(0))\n",
    "        test_loss += binary_loss(outputs, batch_y)\n",
    "\n",
    "        prediction = outputs.data.max(1, keepdim=True)[1]\n",
    "        #label = batch_y.data.max(1, keepdim=True)[1]\n",
    "        label = batch['y'].data\n",
    "        correct += prediction.eq(torch.LongTensor(label)).sum()\n",
    "    \n",
    "    test_loss /= len(testing_loader.dataset)\n",
    "    accuracy = 100. * correct / len(testing_loader.dataset)\n",
    "    print(f'Average Test loss: {test_loss.data[0]}')\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    print('-'*10)\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "    #for batch in tqdm(training_loader):\n",
    "    for batch in training_loader:\n",
    "        # Get the inputs and wrap as Variables\n",
    "        batch_x = Variable(batch[\"x\"])\n",
    "        #batch_y = Variable(batch[\"y\"])\n",
    "        batch_y = Variable(batch['y'].reshape(1,-1).squeeze(0))\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "#         print(\"outputs.shape:\",outputs.shape)\n",
    "#         print(\"batch_y.shape:\",batch_y.shape)\n",
    "#         print(\"outputs\",outputs)\n",
    "#         print(\"batch_y\",batch_y)\n",
    "        \n",
    "        loss = binary_loss(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: (25000, 300)\n",
      "y.shape: (25000,)\n",
      "x.shape: (25000, 300)\n",
      "y.shape: (25000,)\n"
     ]
    }
   ],
   "source": [
    "training_data = MyData(x_train, y_train)\n",
    "testing_data = MyData(x_test, y_test)\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=5)\n",
    "testing_loader = DataLoader(testing_data, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchFastText(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_features, embedding_dims, maxlen, num_classes=2):\n",
    "        super(TorchFastText, self).__init__()\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.maxlen = maxlen\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "#         self.embeds = nn.EmbeddingBag(max_features, embedding_dims)\n",
    "#         self.linear = nn.Linear(self.embedding_dims, self.num_classes)\n",
    "        \n",
    "        self.embeds = nn.Embedding(max_features, embedding_dims)\n",
    "        self.linear = nn.Linear(self.embedding_dims, self.num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded_sentence = self.embeds(x)\n",
    "#         print('embeded.shape:',embedded_sentence.shape)\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded_sentence, (embedded_sentence.shape[1], 1)).squeeze(1)         \n",
    "#         print('pooled.shape:',pooled.shape)\n",
    "        \n",
    "        predicted = self.linear(pooled)\n",
    "#         print('predicted.shape:',predicted.shape)\n",
    "        \n",
    "        return predicted\n",
    "\n",
    "\n",
    "model = TorchFastText(MAX_FEATURE, EMBEDDING_DIMS, SENTENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.5569,  0.3345,  0.0683,  ...,  0.0375, -0.5230,  0.5233],\n",
       "        [ 0.0453,  0.3146,  0.6410,  ..., -0.1689, -1.0540,  0.4726],\n",
       "        [ 0.3994,  0.5463,  0.3801,  ...,  0.4579, -0.1834,  0.1226]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeds.weight.data.copy_(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "#binary_loss = nn.BCELoss()\n",
    "binary_loss = nn.CrossEntropyLoss()\n",
    "# optimizer = Adagrad(model.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/5000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 5/5000 [00:00<02:00, 41.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 7/5000 [00:00<06:16, 13.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 13/5000 [00:00<04:48, 17.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 16/5000 [00:01<07:08, 11.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 18/5000 [00:01<08:17, 10.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 26/5000 [00:01<06:10, 13.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 30/5000 [00:01<04:59, 16.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 35/5000 [00:01<04:03, 20.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 39/5000 [00:01<03:34, 23.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 45/5000 [00:01<03:00, 27.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 49/5000 [00:02<03:53, 21.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 53/5000 [00:02<06:54, 11.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 56/5000 [00:03<06:01, 13.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 59/5000 [00:03<05:19, 15.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  1%|          | 62/5000 [00:03<04:44, 17.34it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 75/5000 [00:03<03:32, 23.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 80/5000 [00:03<03:19, 24.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 85/5000 [00:03<03:42, 22.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 89/5000 [00:04<04:22, 18.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 92/5000 [00:04<04:28, 18.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 104/5000 [00:04<03:21, 24.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  2%|▏         | 118/5000 [00:04<02:31, 32.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 145/5000 [00:04<01:53, 42.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 156/5000 [00:05<02:01, 39.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 165/5000 [00:05<01:54, 42.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  3%|▎         | 173/5000 [00:05<01:50, 43.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 180/5000 [00:05<01:58, 40.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▎         | 186/5000 [00:05<02:07, 37.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 191/5000 [00:05<02:06, 38.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 197/5000 [00:06<01:58, 40.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 202/5000 [00:06<03:25, 23.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 207/5000 [00:06<02:56, 27.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 211/5000 [00:06<03:14, 24.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 215/5000 [00:06<03:13, 24.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 219/5000 [00:07<03:18, 24.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  4%|▍         | 223/5000 [00:07<03:02, 26.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 228/5000 [00:07<02:52, 27.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 234/5000 [00:07<02:36, 30.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 238/5000 [00:07<02:33, 31.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 242/5000 [00:07<03:08, 25.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 245/5000 [00:08<03:29, 22.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▍         | 249/5000 [00:08<03:06, 25.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 252/5000 [00:08<03:02, 26.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 255/5000 [00:08<07:58,  9.91it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 258/5000 [00:09<09:22,  8.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 261/5000 [00:09<07:21, 10.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 264/5000 [00:09<06:01, 13.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  5%|▌         | 270/5000 [00:09<04:46, 16.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 278/5000 [00:09<03:38, 21.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 283/5000 [00:10<03:05, 25.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 291/5000 [00:10<02:28, 31.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 297/5000 [00:10<02:18, 33.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 302/5000 [00:10<02:49, 27.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 307/5000 [00:10<03:37, 21.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▌         | 311/5000 [00:11<03:57, 19.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 314/5000 [00:11<03:42, 21.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 319/5000 [00:11<03:12, 24.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  6%|▋         | 323/5000 [00:11<03:08, 24.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 327/5000 [00:11<02:48, 27.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 331/5000 [00:11<03:09, 24.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  7%|▋         | 355/5000 [00:11<02:17, 33.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 382/5000 [00:12<01:41, 45.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  8%|▊         | 410/5000 [00:12<01:15, 60.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  9%|▉         | 446/5000 [00:12<00:56, 81.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 10%|▉         | 499/5000 [00:12<00:41, 108.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 12%|█▏        | 588/5000 [00:12<00:30, 147.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 16%|█▌        | 792/5000 [00:12<00:20, 203.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 20%|██        | 1007/5000 [00:12<00:14, 279.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 24%|██▍       | 1203/5000 [00:12<00:10, 376.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 29%|██▊       | 1436/5000 [00:12<00:07, 502.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 33%|███▎      | 1631/5000 [00:12<00:05, 646.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 37%|███▋      | 1851/5000 [00:13<00:03, 819.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 41%|████      | 2050/5000 [00:13<00:02, 989.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 45%|████▍     | 2242/5000 [00:13<00:02, 1142.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 49%|████▊     | 2431/5000 [00:13<00:02, 1117.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 52%|█████▏    | 2595/5000 [00:13<00:01, 1229.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 55%|█████▌    | 2758/5000 [00:13<00:01, 1168.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 58%|█████▊    | 2904/5000 [00:13<00:01, 1087.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 61%|██████    | 3046/5000 [00:13<00:01, 1168.98it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████▎   | 3180/5000 [00:14<00:01, 1193.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████▋   | 3325/5000 [00:14<00:01, 1258.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 72%|███████▏  | 3577/5000 [00:14<00:00, 1480.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 75%|███████▌  | 3766/5000 [00:14<00:00, 1579.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 80%|████████  | 4019/5000 [00:14<00:00, 1778.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 84%|████████▍ | 4218/5000 [00:14<00:00, 1814.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████▊ | 4414/5000 [00:14<00:00, 1804.40it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████▏| 4623/5000 [00:14<00:00, 1881.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      " 96%|█████████▋| 4819/5000 [00:14<00:00, 1885.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 5000/5000 [00:14<00:00, 333.60it/s] \u001b[A\u001b[A\u001b[A\u001b[A/home/lirui/.virtualenvs/py36/local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 0.06716606765985489\n",
      "Accuracy: 85\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    train(i)\n",
    "    test()\n",
    "# test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  瞎测试...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch in testing_loader:\n",
    "        batch_x = Variable(batch[\"x\"])\n",
    "        outputs = model(batch_x)\n",
    "        batch_y = Variable(batch['y'].reshape(1,-1).squeeze(0))\n",
    "        prediction = outputs.data.max(1, keepdim=True)[1]\n",
    "        \n",
    "        test_loss += binary_loss(outputs, batch_y)\n",
    "        label = batch['y'].data\n",
    "        correct += prediction.eq(torch.LongTensor(label)).sum()\n",
    "    \n",
    "    test_loss /= len(testing_loader.dataset)\n",
    "    accuracy = 100. * correct / len(testing_loader.dataset)\n",
    "    print(f'Average Test loss: {test_loss.data[0]}')\n",
    "    print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test loss: 0.06716606765985489\n",
      "Accuracy: 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lirui/.virtualenvs/py36/local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lirui/.virtualenvs/py36/local/lib/python3.6/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type TorchFastText. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "# 保存整个模型\n",
    "torch.save(model, 'test_fasttext_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
