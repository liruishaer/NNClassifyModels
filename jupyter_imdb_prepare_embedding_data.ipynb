{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.utils import to_categorical\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS   = 10000  #15000\n",
    "EMBEDDING_DIM   = 300    \n",
    "MAX_SEQ_LENGTH  = 256    #500\n",
    "USE_GLOVE       = True\n",
    "FILTER_SIZES    = [3,4,5]\n",
    "FEATURE_MAPS    = [200,200,200]\n",
    "DROPOUT_RATE    = 0.4\n",
    "HIDDEN_UNITS    = 200\n",
    "NB_CLASSES      = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据获取（保证样本均衡）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载imdb数据集（训练集/测试集）\n",
    "def load_data(path='imdb.npz', num_words=None, skip_top=0, seed=113,\n",
    "              start_char=1, oov_char=2, index_from=3):\n",
    "\n",
    "    # 1. load data\n",
    "    with np.load(path) as f:\n",
    "        x_train, labels_train = f['x_train'], f['y_train']\n",
    "        x_test, labels_test = f['x_test'], f['y_test']\n",
    "\n",
    "    # 2. shuffle train/test\n",
    "    np.random.seed(seed)\n",
    "    indices = np.arange(len(x_train))\n",
    "    np.random.shuffle(indices)\n",
    "    x_train = x_train[indices]\n",
    "    labels_train = labels_train[indices]\n",
    "\n",
    "    indices = np.arange(len(x_test))\n",
    "    np.random.shuffle(indices)\n",
    "    x_test = x_test[indices]\n",
    "    labels_test = labels_test[indices]\n",
    "\n",
    "    xs = np.concatenate([x_train, x_test])\n",
    "    labels = np.concatenate([labels_train, labels_test])\n",
    "\n",
    "    # 保留前3个index\n",
    "    if start_char is not None:\n",
    "        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n",
    "    elif index_from:\n",
    "        xs = [[w + index_from for w in x] for x in xs]\n",
    "\n",
    "\n",
    "    if not num_words:\n",
    "        num_words = max([max(x) for x in xs])\n",
    "\n",
    "    # by convention, use 2 as OOV word\n",
    "    # reserve 'index_from' (=3 by default) characters:\n",
    "    # 0 (padding), 1 (start), 2 (OOV)\n",
    "    if oov_char is not None:\n",
    "        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x] for x in xs]\n",
    "    else:\n",
    "        xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n",
    "\n",
    "    idx = len(x_train)\n",
    "    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
    "    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 加载：单词-index字典\n",
    "def get_word_index(path='imdb_word_index.json'):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=MAX_NUM_WORDS)\n",
    "len(train_data)\n",
    "\n",
    "# all_data = np.hstack([train_data,test_data])\n",
    "# all_labels = np.hstack([train_labels,test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def split_data(data_length_list = [10,20]):\n",
    "    sfolder = StratifiedKFold(n_splits=4,random_state=0,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS   = 10000  #15000\n",
    "EMBEDDING_DIM   = 100    # 50,100,200,300\n",
    "MAX_SEQ_LENGTH  = 256    #500\n",
    "USE_GLOVE       = True\n",
    "FILTER_SIZES    = [3,4,5]\n",
    "FEATURE_MAPS    = [200,200,200]\n",
    "DROPOUT_RATE    = 0.4\n",
    "HIDDEN_UNITS    = 200\n",
    "NB_CLASSES      = 1   # 2\n",
    "\n",
    "# LEARNING\n",
    "BATCH_SIZE      = 100\n",
    "NB_EPOCHS       = 10\n",
    "RUNS            = 2\n",
    "VAL_SIZE        = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary mapping words to an integer index\n",
    "word_index = imdb.get_word_index()  # {word:index}\n",
    "\n",
    "# The first indices are reserved\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(len(word_index))\n",
    "tmp_dict = {wd:idx for wd,idx in word_index.items() if len(wd.split())>1}\n",
    "# print(tmp_dict)\n",
    "# tmp_dict\n",
    "len(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data,\n",
    "                                                        value=word_index[\"<PAD>\"],\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_SEQ_LENGTH)\n",
    "\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data,\n",
    "                                                       value=word_index[\"<PAD>\"],\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 256)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[0]), len(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_embeddings():\n",
    "    print('Pretrained embeddings GloVe is loading...')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open('/liruishaer/Work2/NLP_models/glove.6B/glove.6B.%id.txt' % EMBEDDING_DIM)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors in GloVe embedding' % len(embeddings_index))\n",
    "\n",
    "    embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_NUM_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return Embedding(\n",
    "        input_dim=MAX_NUM_WORDS,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        input_length=MAX_SEQ_LENGTH,\n",
    "        weights=[embedding_matrix],\n",
    "        trainable=True,\n",
    "        name=\"word_embedding\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors in GloVe embedding\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('/liruishaer/Work2/NLP_models/glove.6B/glove.6B.%id.txt' % EMBEDDING_DIM)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors in GloVe embedding' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((MAX_NUM_WORDS, EMBEDDING_DIM))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.55689001,  0.33454001,  0.068255  , ...,  0.037454  ,\n",
       "        -0.52304   ,  0.52328998],\n",
       "       [ 0.045259  ,  0.31463999,  0.64099002, ..., -0.1689    ,\n",
       "        -1.05400002,  0.47262999],\n",
       "       [ 0.39943001,  0.54632998,  0.38009   , ...,  0.45795   ,\n",
       "        -0.18339001,  0.12257   ]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN model for text classification\n",
    "This implementation is based on the original paper of Yoon Kim [1].\n",
    "# References\n",
    "- [1] [Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)\n",
    "@author: Christopher Masch\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import Activation, Input, Dense, Dropout, Embedding\n",
    "from keras.layers.convolutional import SeparableConv1D\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "\n",
    "class CNN:\n",
    "    \n",
    "    __version__ = '0.0.2'\n",
    "    \n",
    "    def __init__(self, embedding_layer=None, num_words=None, embedding_dim=None,\n",
    "                 max_seq_length=100, filter_sizes=[3,4,5], feature_maps=[100,100,100],\n",
    "                 hidden_units=100, dropout_rate=None, nb_classes=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            embedding_layer : If not defined with pre-trained embeddings it will be created from scratch (default: None)\n",
    "            num_words       : Maximal amount of words in the vocabulary (default: None)\n",
    "            embedding_dim   : Dimension of word representation (default: None)\n",
    "            max_seq_length  : Max length of sequence (default: 100)\n",
    "            filter_sizes    : An array of filter sizes per channel (default: [3,4,5])\n",
    "            feature_maps    : Defines the feature maps per channel (default: [100,100,100])\n",
    "            hidden_units    : Hidden units per convolution channel (default: 100)\n",
    "            dropout_rate    : If defined, dropout will be added after embedding layer & concatenation (default: None)\n",
    "            nb_classes      : Number of classes which can be predicted\n",
    "        \"\"\"\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.num_words       = num_words\n",
    "        self.max_seq_length  = max_seq_length\n",
    "        self.embedding_dim   = embedding_dim\n",
    "        self.filter_sizes    = filter_sizes\n",
    "        self.feature_maps    = feature_maps\n",
    "        self.hidden_units    = hidden_units\n",
    "        self.dropout_rate    = dropout_rate\n",
    "        self.nb_classes      = nb_classes\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build the model\n",
    "        \n",
    "        Returns:\n",
    "            Model           : Keras model instance\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks\n",
    "        if len(self.filter_sizes)!=len(self.feature_maps):\n",
    "            raise Exception('Please define `filter_sizes` and `feature_maps` with the same length.')\n",
    "        if not self.embedding_layer and (not self.num_words or not self.embedding_dim):\n",
    "            raise Exception('Please define `num_words` and `embedding_dim` if you not use a pre-trained embeddings')\n",
    "        \n",
    "        \n",
    "        # Building embeddings from scratch\n",
    "        if self.embedding_layer is None:\n",
    "            self.embedding_layer = Embedding(\n",
    "                input_dim=self.num_words, \n",
    "                output_dim=self.embedding_dim,       \n",
    "                input_length=self.max_seq_length,\n",
    "                weights=None, trainable=True,\n",
    "                name=\"word_embedding\"\n",
    "            )\n",
    "        \n",
    "        word_input = Input(shape=(self.max_seq_length,), dtype='int32', name='word_input')\n",
    "        x = self.embedding_layer(word_input)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        x = self.building_block(x, self.filter_sizes, self.feature_maps)\n",
    "        x = Activation('relu')(x)\n",
    "        prediction = Dense(self.nb_classes, activation='softmax')(x)\n",
    "        return Model(inputs=word_input, outputs=prediction)\n",
    "    \n",
    "    \n",
    "    def building_block(self, input_layer, filter_sizes, feature_maps):\n",
    "        \"\"\" \n",
    "        Creates several CNN channels in parallel and concatenate them \n",
    "        \n",
    "        Arguments:\n",
    "            input_layer : Layer which will be the input for all convolutional blocks\n",
    "            filter_sizes: Array of filter sizes\n",
    "            feature_maps: Array of feature maps\n",
    "            \n",
    "        Returns:\n",
    "            x           : Building block with one or several channels\n",
    "        \"\"\"\n",
    "        channels = []\n",
    "        for ix in range(len(self.filter_sizes)):\n",
    "            x = self.create_channel(input_layer, filter_sizes[ix], feature_maps[ix])\n",
    "            channels.append(x)\n",
    "            \n",
    "        # Checks how many channels, one channel doesn't need a concatenation\n",
    "        if (len(channels)>1):\n",
    "            x = concatenate(channels)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def create_channel(self, x, filter_size, feature_map):\n",
    "        \"\"\"\n",
    "        Creates a layer, working channel wise\n",
    "        \n",
    "        Arguments:\n",
    "            x           : Input for convoltuional channel\n",
    "            filter_size : Filter size for creating Conv1D\n",
    "            feature_map : Feature map \n",
    "            \n",
    "        Returns:\n",
    "            x           : Channel including (Conv1D + GlobalMaxPooling + Dense + Dropout)\n",
    "        \"\"\"\n",
    "        x = SeparableConv1D(feature_map, kernel_size=filter_size, activation='relu', strides=1, padding='same',\n",
    "                            depth_multiplier=4)(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "        x = Dense(self.hidden_units)(x)\n",
    "        x = Dropout(self.dropout_rate)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 ... 0 1 0]\n",
      "(25000,)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "word_input (InputLayer)         (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 256, 50)      50000       word_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 256, 50)      0           word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_31 (SeparableC (None, 256, 200)     40800       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_32 (SeparableC (None, 256, 200)     41000       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv1d_33 (SeparableC (None, 256, 200)     41200       dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_31 (Global (None, 200)          0           separable_conv1d_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_32 (Global (None, 200)          0           separable_conv1d_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_33 (Global (None, 200)          0           separable_conv1d_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 200)          40200       global_max_pooling1d_31[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 200)          40200       global_max_pooling1d_32[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 200)          40200       global_max_pooling1d_33[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 200)          0           dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_43 (Dropout)            (None, 200)          0           dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_44 (Dropout)            (None, 200)          0           dense_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 600)          0           dropout_42[0][0]                 \n",
      "                                                                 dropout_43[0][0]                 \n",
      "                                                                 dropout_44[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 600)          0           concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 1)            601         activation_11[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 294,201\n",
      "Trainable params: 294,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 107s 4ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 7.97119, saving model to model-1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lirui/.virtualenvs/py36/local/lib/python3.6/site-packages/keras/models.py:124: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "21500/25000 [========================>.....] - ETA: 12s - loss: 7.9653 - acc: 0.5004"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-090734b84c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     callbacks=[\n\u001b[1;32m     63\u001b[0m         ModelCheckpoint(\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;34m'model-%i.h5'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         ),\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#TensorBoard(log_dir='./logs/temp', write_graph=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/py36/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "histories = []\n",
    "\n",
    "\n",
    "# print('Running iteration %i/%i' % (i+1, RUNS))\n",
    "# random_state = np.random.randint(1000)\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=VAL_SIZE, random_state=random_state)\n",
    "X_train = train_data\n",
    "X_val = test_data\n",
    "\n",
    "# train_labels = to_categorical(train_labels)\n",
    "# train_labels = to_categorical(test_labels)\n",
    "y_train = train_labels\n",
    "y_val = test_labels\n",
    "\n",
    "print(y_train)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "emb_layer = None\n",
    "# if USE_GLOVE:\n",
    "#     emb_layer = create_glove_embeddings()\n",
    "\n",
    "model = CNN(\n",
    "    embedding_layer = emb_layer,\n",
    "    num_words       = MAX_NUM_WORDS,\n",
    "    embedding_dim   = EMBEDDING_DIM,\n",
    "    filter_sizes    = FILTER_SIZES,\n",
    "    feature_maps    = FEATURE_MAPS,\n",
    "    max_seq_length  = MAX_SEQ_LENGTH,\n",
    "    dropout_rate    = DROPOUT_RATE,\n",
    "    hidden_units    = HIDDEN_UNITS,\n",
    "    nb_classes      = NB_CLASSES\n",
    ").build_model()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# binary_crossentropy  :  integer label    NB_CLASSED=1\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# categoricla_crossentropy  : binary label    NB_CLASSED=2\n",
    "# model.compile(\n",
    "#     loss='categorical_crossentropy',\n",
    "#     optimizer=optimizers.Adam(),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=NB_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    verbose=1,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(\n",
    "            'model-%i.h5'%(1), monitor='val_loss', verbose=1, save_best_only=True, mode='min'\n",
    "        ),\n",
    "        #TensorBoard(log_dir='./logs/temp', write_graph=True)\n",
    "    ]\n",
    ")\n",
    "print()\n",
    "histories.append(history.history)\n",
    "\n",
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(histories, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
